{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and move to dataset directory\n",
    "import os\n",
    "# import the needed packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from datetime import datetime\n",
    "import itertools \n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = 'C:\\\\Users\\\\sazid\\\\Desktop\\\\jour_data\\\\toad_background_CNN1\\\\train'\n",
    "valPath = 'C:\\\\Users\\\\sazid\\\\Desktop\\\\jour_data\\\\toad_background_CNN1\\\\valid'\n",
    "testPath = 'C:\\\\Users\\\\sazid\\\\Desktop\\\\jour_data\\\\toad_background_CNN1\\\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "# initialize the validation/testing data augmentation object (which\n",
    "# we'll be adding mean subtraction to)\n",
    "valAug = ImageDataGenerator()\n",
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation\n",
    "# objects\n",
    "testAug=ImageDataGenerator()\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean\n",
    "testAug.mean = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n",
      "Found 500 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# initialize the training generator\n",
    "batch_size = 32\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "    trainPath,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "    valPath,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)\n",
    "# initialize the testing generator\n",
    "testGen = testAug.flow_from_directory(\n",
    "    testPath,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 21,137,986\n",
      "Trainable params: 13,502,722\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "0 input_1 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 False\n",
      "8 block3_conv2 False\n",
      "9 block3_conv3 False\n",
      "10 block3_pool False\n",
      "11 block4_conv1 False\n",
      "12 block4_conv2 False\n",
      "13 block4_conv3 False\n",
      "14 block4_pool False\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "19 flatten True\n",
      "20 dense True\n",
      "21 dropout True\n",
      "22 dense_1 True\n"
     ]
    }
   ],
   "source": [
    "# load the VGG16 network, ensuring the head FC layer sets are left\n",
    "# off\n",
    "#VGG16 = keras.applications.vgg16\n",
    "from keras.layers import Input\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "\n",
    "# Freeze four convolution blocks\n",
    "for layer in baseModel.layers[:15]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "headModel = baseModel.output\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(256, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "#headModel = Dense(256, activation=\"relu\")(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "model.summary() \n",
    "\n",
    "# Make sure you have frozen the correct layers\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model...\n",
      "training head...\n",
      "Epoch 1/100\n",
      "78/78 [==============================] - 125s 2s/step - loss: 1.8805 - acc: 0.7530 - val_loss: 0.3021 - val_acc: 0.8604\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 109s 1s/step - loss: 0.2742 - acc: 0.8959 - val_loss: 0.1707 - val_acc: 0.9375\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.2082 - acc: 0.9222 - val_loss: 0.1369 - val_acc: 0.9500\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 109s 1s/step - loss: 0.1738 - acc: 0.9373 - val_loss: 0.0948 - val_acc: 0.9708\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 109s 1s/step - loss: 0.1384 - acc: 0.9536 - val_loss: 0.0919 - val_acc: 0.9771\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 108s 1s/step - loss: 0.1145 - acc: 0.9643 - val_loss: 0.0503 - val_acc: 0.9937\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 108s 1s/step - loss: 0.0899 - acc: 0.9711 - val_loss: 0.0486 - val_acc: 0.9896\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 108s 1s/step - loss: 0.0854 - acc: 0.9811 - val_loss: 0.0460 - val_acc: 0.9937\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 108s 1s/step - loss: 0.0890 - acc: 0.9735 - val_loss: 0.0369 - val_acc: 0.9937\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 108s 1s/step - loss: 0.0564 - acc: 0.9852 - val_loss: 0.0423 - val_acc: 0.9896\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 108s 1s/step - loss: 0.0542 - acc: 0.9784 - val_loss: 0.0394 - val_acc: 0.9896\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0645 - acc: 0.9842 - val_loss: 0.0423 - val_acc: 0.9958\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0649 - acc: 0.9797 - val_loss: 0.0288 - val_acc: 0.9937\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0499 - acc: 0.9863 - val_loss: 0.0284 - val_acc: 0.9937\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0465 - acc: 0.9851 - val_loss: 0.0268 - val_acc: 0.9937\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0557 - acc: 0.9818 - val_loss: 0.0340 - val_acc: 0.9958\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0399 - acc: 0.9887 - val_loss: 0.0388 - val_acc: 0.9937\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 109s 1s/step - loss: 0.0317 - acc: 0.9894 - val_loss: 0.0216 - val_acc: 0.9958\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0357 - acc: 0.9897 - val_loss: 0.0225 - val_acc: 0.9937\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0386 - acc: 0.9877 - val_loss: 0.0285 - val_acc: 0.9958\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0422 - acc: 0.9855 - val_loss: 0.0208 - val_acc: 0.9958\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0301 - acc: 0.9912 - val_loss: 0.0197 - val_acc: 0.9937\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0252 - acc: 0.9896 - val_loss: 0.0203 - val_acc: 0.9958\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0278 - acc: 0.9921 - val_loss: 0.0176 - val_acc: 0.9958\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.0168 - val_acc: 0.9979\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0243 - acc: 0.9934 - val_loss: 0.0181 - val_acc: 0.9937\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0199 - acc: 0.9956 - val_loss: 0.0158 - val_acc: 0.9958\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0210 - acc: 0.9942 - val_loss: 0.0137 - val_acc: 0.9937\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0363 - acc: 0.9867 - val_loss: 0.0213 - val_acc: 0.9979\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0112 - val_acc: 0.9958\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0121 - acc: 0.9966 - val_loss: 0.0148 - val_acc: 0.9958\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 112s 1s/step - loss: 0.0207 - acc: 0.9941 - val_loss: 0.0165 - val_acc: 0.9937\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 112s 1s/step - loss: 0.0236 - acc: 0.9900 - val_loss: 0.0155 - val_acc: 0.9958\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0133 - acc: 0.9960 - val_loss: 0.0138 - val_acc: 0.9979\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 112s 1s/step - loss: 0.0145 - acc: 0.9945 - val_loss: 0.0204 - val_acc: 0.9979\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 112s 1s/step - loss: 0.0138 - acc: 0.9979 - val_loss: 0.0098 - val_acc: 0.9958\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0145 - acc: 0.9948 - val_loss: 0.0162 - val_acc: 0.9958\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 110s 1s/step - loss: 0.0199 - acc: 0.9926 - val_loss: 0.0111 - val_acc: 0.9958\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0166 - acc: 0.9968 - val_loss: 0.0133 - val_acc: 0.9979\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 111s 1s/step - loss: 0.0120 - acc: 0.9974 - val_loss: 0.0126 - val_acc: 0.9979\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0121 - val_acc: 0.9958\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 112s 1s/step - loss: 0.0124 - acc: 0.9949 - val_loss: 0.0125 - val_acc: 0.9979\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0136 - acc: 0.9981 - val_loss: 0.0114 - val_acc: 0.9979\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0128 - acc: 0.9964 - val_loss: 0.0100 - val_acc: 0.9979\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0080 - acc: 0.9986 - val_loss: 0.0114 - val_acc: 0.9979\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0094 - val_acc: 0.9979\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.0109 - val_acc: 0.9979\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0113 - acc: 0.9954 - val_loss: 0.0067 - val_acc: 0.9979\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0154 - acc: 0.9929 - val_loss: 0.0101 - val_acc: 0.9979\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0093 - acc: 0.9960 - val_loss: 0.0117 - val_acc: 0.9979\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0135 - acc: 0.9953 - val_loss: 0.0131 - val_acc: 0.9979\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0068 - acc: 0.9986 - val_loss: 0.0133 - val_acc: 0.9979\n",
      "Epoch 53/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0069 - acc: 0.9969 - val_loss: 0.0079 - val_acc: 0.9958\n",
      "Epoch 54/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.0085 - val_acc: 0.9979\n",
      "Epoch 55/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0083 - acc: 0.9984 - val_loss: 0.0111 - val_acc: 0.9979\n",
      "Epoch 56/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0078 - acc: 0.9971 - val_loss: 0.0096 - val_acc: 0.9979\n",
      "Epoch 57/100\n",
      "78/78 [==============================] - 112s 1s/step - loss: 0.0097 - acc: 0.9978 - val_loss: 0.0112 - val_acc: 0.9979\n",
      "Epoch 58/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9979\n",
      "Epoch 59/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0099 - val_acc: 0.9979\n",
      "Epoch 60/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0086 - val_acc: 0.9979\n",
      "Epoch 61/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0090 - acc: 0.9946 - val_loss: 0.0055 - val_acc: 0.9958\n",
      "Epoch 62/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0177 - acc: 0.9950 - val_loss: 0.0131 - val_acc: 0.9979\n",
      "Epoch 63/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0086 - acc: 0.9986 - val_loss: 0.0079 - val_acc: 0.9979\n",
      "Epoch 64/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0044 - acc: 0.9996 - val_loss: 0.0093 - val_acc: 0.9979\n",
      "Epoch 65/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0071 - acc: 0.9963 - val_loss: 0.0135 - val_acc: 0.9979\n",
      "Epoch 66/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0050 - val_acc: 0.9979\n",
      "Epoch 67/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0203 - acc: 0.9934 - val_loss: 0.0091 - val_acc: 0.9979\n",
      "Epoch 68/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0062 - val_acc: 0.9979\n",
      "Epoch 69/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0125 - val_acc: 0.9979\n",
      "Epoch 70/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0086 - acc: 0.9962 - val_loss: 0.0075 - val_acc: 0.9979\n",
      "Epoch 71/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0075 - acc: 0.9987 - val_loss: 0.0093 - val_acc: 0.9979\n",
      "Epoch 72/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0055 - acc: 0.9995 - val_loss: 0.0089 - val_acc: 0.9979\n",
      "Epoch 73/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0047 - val_acc: 0.9979\n",
      "Epoch 74/100\n",
      "78/78 [==============================] - 121s 2s/step - loss: 0.0080 - acc: 0.9966 - val_loss: 0.0129 - val_acc: 0.9979\n",
      "Epoch 75/100\n",
      "78/78 [==============================] - 118s 2s/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9979\n",
      "Epoch 76/100\n",
      "78/78 [==============================] - 119s 2s/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0068 - val_acc: 0.9979\n",
      "Epoch 77/100\n",
      "78/78 [==============================] - 120s 2s/step - loss: 0.0054 - acc: 0.9969 - val_loss: 0.0072 - val_acc: 0.9979\n",
      "Epoch 78/100\n",
      "78/78 [==============================] - 119s 2s/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0084 - val_acc: 0.9979\n",
      "Epoch 79/100\n",
      "78/78 [==============================] - 119s 2s/step - loss: 0.0037 - acc: 0.9982 - val_loss: 0.0099 - val_acc: 0.9979\n",
      "Epoch 80/100\n",
      "78/78 [==============================] - 116s 1s/step - loss: 0.0089 - acc: 0.9956 - val_loss: 0.0034 - val_acc: 0.9979\n",
      "Epoch 81/100\n",
      "78/78 [==============================] - 117s 1s/step - loss: 0.0079 - acc: 0.9967 - val_loss: 0.0082 - val_acc: 0.9979\n",
      "Epoch 82/100\n",
      "78/78 [==============================] - 116s 1s/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0194 - val_acc: 0.9979\n",
      "Epoch 83/100\n",
      "78/78 [==============================] - 116s 1s/step - loss: 0.0065 - acc: 0.9995 - val_loss: 0.0103 - val_acc: 0.9979\n",
      "Epoch 84/100\n",
      "78/78 [==============================] - 117s 1s/step - loss: 0.0052 - acc: 0.9980 - val_loss: 0.0039 - val_acc: 0.9979\n",
      "Epoch 85/100\n",
      "78/78 [==============================] - 117s 1s/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9979\n",
      "Epoch 86/100\n",
      "78/78 [==============================] - 118s 2s/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0099 - val_acc: 0.9979\n",
      "Epoch 87/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0047 - acc: 0.9976 - val_loss: 0.0106 - val_acc: 0.9979\n",
      "Epoch 88/100\n",
      "78/78 [==============================] - 116s 1s/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9979\n",
      "Epoch 89/100\n",
      "78/78 [==============================] - 117s 2s/step - loss: 0.0099 - acc: 0.9972 - val_loss: 0.0059 - val_acc: 0.9979\n",
      "Epoch 90/100\n",
      "78/78 [==============================] - 115s 1s/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0094 - val_acc: 0.9979\n",
      "Epoch 91/100\n",
      "78/78 [==============================] - 116s 1s/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 0.9979\n",
      "Epoch 92/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.0120 - val_acc: 0.9979\n",
      "Epoch 93/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0072 - val_acc: 0.9979\n",
      "Epoch 94/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 0.9979\n",
      "Epoch 95/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0027 - acc: 0.9987 - val_loss: 0.0048 - val_acc: 0.9979\n",
      "Epoch 96/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0091 - val_acc: 0.9979\n",
      "Epoch 97/100\n",
      "78/78 [==============================] - 113s 1s/step - loss: 0.0024 - acc: 0.9998 - val_loss: 0.0060 - val_acc: 0.9979\n",
      "Epoch 98/100\n",
      "78/78 [==============================] - 114s 1s/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0063 - val_acc: 0.9979\n",
      "Epoch 99/100\n",
      "78/78 [==============================] - 116s 1s/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9979\n",
      "Epoch 100/100\n",
      "78/78 [==============================] - 118s 2s/step - loss: 9.0128e-04 - acc: 1.0000 - val_loss: 0.0109 - val_acc: 0.9979\n"
     ]
    }
   ],
   "source": [
    "# compile our model (this needs to be done after our setting our\n",
    "# layers to being non-trainable\n",
    "\n",
    "totalTrain=2506\n",
    "totalVal=500\n",
    "print(\"compiling model...\")\n",
    "opt = SGD(lr=1e-4, momentum=0.9)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"acc\"])\n",
    "# train the head of the network for a few epochs (all other layers\n",
    "# are frozen) -- this will allow the new FC layers to start to become\n",
    "# initialized with actual \"learned\" values versus pure random\n",
    "print(\"training head...\")\n",
    "H = model.fit(\n",
    "    x=trainGen,\n",
    "    steps_per_epoch=totalTrain // batch_size,\n",
    "    validation_data=valGen,\n",
    "    validation_steps=totalVal // batch_size,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"toad2_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background', 'toad']\n"
     ]
    }
   ],
   "source": [
    "target_names = []\n",
    "for key in trainGen.class_indices:\n",
    "    target_names.append(key)\n",
    "    \n",
    "print(target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9b67da3524f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Plot the Graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#Plot the Graph\n",
    "\n",
    "acc = H.history['acc']\n",
    "val_acc = H.history['val_acc']\n",
    "loss = H.history['loss']\n",
    "val_loss = H.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Accuracy Curves\n",
    "\n",
    "plt.figure(1)\n",
    "# summarize history for accuracy\n",
    "plt.plot(H.history['acc'])\n",
    "plt.plot(H.history['val_acc'])\n",
    "plt.title('Model accuracy of Toad Vs Backgroud')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.savefig('acc.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "# summarize history for loss\n",
    "plt.plot(H.history['loss'])\n",
    "plt.plot(H.history['val_loss'])\n",
    "plt.title('Model loss of Toad Vs Backgroud')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.savefig('loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating after fine-tuning network head...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.99      0.98      0.98       100\n",
      "        toad       0.98      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.99      0.98      0.98       200\n",
      "weighted avg       0.99      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reset the testing generator and evaluate the network after\n",
    "# fine-tuning just the network head\n",
    "print(\"evaluating after fine-tuning network head...\")\n",
    "\n",
    "totalTest=200\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(x=testGen, steps=(totalTest // batch_size) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs,\n",
    "    target_names=testGen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix of snake Vs background', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    #plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=2)\n",
    "        cm[np.isnan(cm)] = 0.0\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "   \n",
    "\n",
    "#thresh = cm.max() / 2.\n",
    "#Matplotlibâ€™s matshow\n",
    "cm = confusion_matrix(testGen.classes, predIdxs)\n",
    "\n",
    "plt.figure(3)\n",
    "plot_confusion_matrix(cm, target_names, title='Confusion matrix of Toad Vs Backgroud')\n",
    "#print(cm)\n",
    "plt.savefig('snake_VGG16.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
